# 第三章 预训练模型

## 一、 Encoder-only PLM

### 1. BERT

开启了预训练+微调模型在自然语言处理上的主流时代。

#### 模型架构：

![image-20250918211147578](C:\Users\a\AppData\Roaming\Typora\typora-user-images\image-20250918211147578.png)

其输入一般是文本序列，输出一般是Label。

#### 特殊架构：

1. BERT使用的激活函数是 GELU 函数，全名为高斯误差线性单元激活函数。

$$
GELU(x) = 0.5 x(1 + \tanh{(\sqrt{\frac{2}{\pi}})}(x+0.04715x^3))
$$

将随机正则的思想引入激活函数，通过输入自身的概率分布来决定抛弃还是保留自身的神经元。

2. 将相对位置编码融合在了注意力机制中，将相对位置编码同样视为可训练的权重参数。从而拟合更丰富的相对位置信息。

#### 预训练任务——MLM + NSP：

**预训练-微调范式**的核心优势：通过将预训练和微调分离，完成一次预训练的模型可以仅通过微调应用在几乎所有下游任务上。

**MLM(Masked Language Model):**掩码语言模型。在一个文本序列中随机遮蔽部分 token，要求模型根据输入预测被遮蔽的 token。使得模型可以拟合双向语义，更好地实现文本的理解。

**NSP (Next Sentence Prediction)：**下一个句子预测。通过要求模型判断两个句子是否为连续的上下文从而培养其问答匹配、自然语言推理的能力。

#### 下游任务微调：

在海量无监督语料上进行预训练来获得通用的文本理解与生成能力，再在对应的下游任务上进行微调。

### 2. RoBERTa

#### 优化一：去掉NSP预训练任务

经过实验发现NSP并不能提高模型性能，且对MLM任务进行改进。把 Mask 操作放入了训练阶段，采用动态遮蔽策略，每一个 Epoch 的训练数据 Mask 的位置都不一致。

#### 优化二：更大规模的预训练数据和预训练步长

使用了更大量的无监督预料进行训练，采用更大的batch size来提高优化速度

#### 优化三：更大的 bpe 词表

使用 BPE（Byte Pair Encoding） 作为 Tokenizer 的编码策略。以字词对作为分词的单位。

### 3. ALBERT

#### 优化一：将 Embedding 参数进行分解

Embedding层输出的向量是对文本token的稠密向量表示，但只需100维就能取得很好的效果。因此可在 Embedding 层和隐藏层之间加入一个线性矩阵进行维度变换，从而实现参数量的减少。

#### 优化二：跨层进行参数共享

24个 Encoder 共用同一个 Encoder 层，将节省下的空间用来扩大隐藏层的维度，实现一个更宽但参数量更小的模型。

但这个修改对训练效率提升不大，却使得训练和推理的速度相比BERT更慢。

#### 优化三：提出SOP预训练任务

针对 NSP 任务提出了改进，SOP 的负例是将两个句子的顺序反过来，从而训练模型不仅要拟合两个句子之间的关系，更要学习其顺序关系，从而提升预训练的难度。实验证明： MLM + SOP 比单独的 MLM 预训练要好上不少。

## 二、Encoder-Decoder PLM

BERT存在着 MLM 任务和下游任务微调的不一致性以及无法处理超过模型训练长度的输入的问题。因此提出了 Encoder-Decoder 模型。

### **T5**

基于 Transformer 架构，包含编码器和解码器两个部分，使用自注意力机制和多头注意力捕捉全局依赖关系，利用相对位置编码处理长序列中的位置信息，并在每层中包含前馈神经网络进一步处理特征。

![图片描述](https://raw.githubusercontent.com/datawhalechina/happy-llm/main/docs/images/3-figures/2-1.png)

#### 特殊实现：

1. T5 模型的 LayerNorm 采用了 RMSNorm，由于参数设置更简单，可以更好地适应不同的任务和数据集。

$$
RMSNorm(x)=\frac{x}{\sqrt{\frac{1}{n}\sum_{i=1}^nx_i^2 + \epsilon}} \cdot \gamma
$$

其中 $x_i$是输入向量的第$i$个元素，$\gamma$是可学习的缩放参数，$n$是输入向量的维度数量，$\epsilon$是一个小常数，避免分母为0.

#### 预训练任务：

1. 预训练任务：采用 MLM，在文本中随机遮蔽15%的 token。
2. 输入格式：将输入文本转换为“文本到文本”的格式，对于一个给定的文本序列，选择一些token进行遮蔽，然后将遮蔽的token序列作为模型的输出目标。
3. 预训练数据集：T5的数据集经过了一定的清洗，去除了无意义的文本、重复文本等
4. 多任务预训练：T5将多个任务混合一起进行预训练，有利于模型学习更通用的语言表示
5. 预训练到微调的转换：微调时在任务特定的数据集上进行训练，并根据任务调整解码策略。

#### 大一统思想

**核心理念：**将所有NLP任务都可以统一为文本到文本的任务。

在每次输入前添加一个任务描述前缀，明确指定当前任务的类型。